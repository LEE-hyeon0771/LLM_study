# LLM parameter 정리

### 1. temperature

- 언어 모델 출력의 무작위성을 제어하는 하이퍼파라미터이다.

- 높을 수록 예측하기 어렵고, 창의적인 결과가 생성되며 모델의 출력에 대한 "신뢰도"가 낮아지게 된다.

- 낮을 수록 결정적이고 보수적인 결과가 생성되므로 예측가능한 출력이 생성된다.


```
예를 들어, 0.5로 temperature를 조정하면, 1.0으로 temperature를 설정할 때보다 더 예측하기 쉽고 덜 창의적인 텍스트가 생성되는 것이다.
애플리케이션의 경우 사실을 기반으로 하는 질의응답과 같은 작업에서는 낮은 temperature를 사용해서 사실적이고 간결한 응답을 얻는 것이 좋고, 시를 생성하는 것과 같은 창의적인 작업에서는 temperature를 높여서 사용하는 것이 좋다.

Open AI API의 경우는 0~2 사이의 값일 수 있으며, Huggingface 모델의 경우 0으로 설정 할 수 없다.
숫자를 temperature로 나누기 때문에 Huggingface에서는 0을 허용하지 않는다.
OpenAI는 0을 가장 높은 확률 옵션만 유지하기 위한 약칭으로 간주한다.

수학적으로 LLM이 다음 토큰의 확률을 계산하기 전에 수행하는 작업이 모든 숫자를 temperature로 나누는 softmax함수의 행위 이므로, 모델이 훈련하는 동안의 temperature는 사실상 1이 된다.
training에서는 계산된 확률을 정확하게 사용해야 하기 때문에 이 시점에서 temperature를 사용하는 것은 큰 의미가 없다.
하지만, 모델을 사용하게 되면 temperature를 변경해서 결과에 영향을 줄 수 있다.
```


### 2. top-k Sampling

- 샘플링을 하는 동안 고려하는 옵션의 수를 제한한다. 가능성이 가장 높은 Top-k 토큰을 선택하여 고품질 출력을 보장하는 하이퍼파라미터이다.

- Top k는 품질이 낮은 무의미한 응답을 피하는데 유용하다.

- Top k는 특수한 경우가 아니라면 0으로 설정하는 것을 권장한다.

```
Huggingface 라이브러리는 생성 구성을 할 때 기본값을 50으로 한다.

1 : 최고의 토큰만을 원하고 결과가 반복 가능함.
50 : 기본 값이고, 상당히 크지만 전혀 예상치 못한 단어가 튀어나오는 것을 방지함.

예를 들어, Top-k를 10으로 설정하게 되면, LLM은 가장 가능성이 높은 다음 단어 10개만을 고려하게 된다.
이렇게 되면, 텍스트 자체의 품질은 높아지지만, 다양성은 줄어들게 된다.
```
 

 

### 3. top-p Sampling (Nucleus Sampling)

- 언어 모델 출력의 무작위성을 제어하는 하이퍼파라미터이다.

- 임계 확률을 설정하고 누적 확률이 임계치를 초과하는 상위 토큰을 선택한다. 그 후 모델이 토큰 세트에서 무작위로 샘플링하여 출력을 생성하게 된다.

- 전체 어휘를 무작위로 샘플링하는 기존 방법보다 더 다양한 출력을 생성할 수 있다.

- Top P는 특수한 경우가 아니라면 0.8~1로 설정하는 것을 권장한다.

```
예를 들어, Top-p가 0.9면 모델은 확률 질량의 90%를 구성하는 가장 가능성이 높은 단어만 고려하게 된다.
이 경우에 응답의 정확도는 낮아지지만 텍스트의 다양성이 높아질 수 있다.

정확하고 사실적인 답변을 원할 경우에 낮게 유지하고, 다양한 반응을 원할 경우 높은 값으로 증가시키는 것이 좋다.

일반적으로 0.0과1.0 사이 숫자로 표현되며, 1.0은 100%, 0은 0%이다.

만약 "word1" 이라는 단어의 확률이 40%이고, "word2"라는 단어의 확률이 20%라면, 상위 P가 0.40인 경우 "word1" 이라는 단어만 고려하게 된다.
상위 P가 0.30이라도 word1만 고려하게 되는 것이다.

일반적으로 temperature과 Top-p를 동시에 수정하지는 않는다.
```

### 4. frequency_penalty

- 자주 사용되는 단어를 생성하는 데 대해 LLM에 페널티를 부과한다.

- LLM이 반복적인 텍스트를 생성하는 것을 방지하는데 유용하다.

- 높은 값으로 반복을 감소시킬 수 있지만, 너무 높으면 자연스러움이 떨어질 수 있다.


### 5. presence_penalty

- 생성된 텍스트에서 이전에 나타난 단어나 구를 재사용하는 것에 대해 페널티를 부과한다.

- LLM이 동일한 단어나 구를 반복해서 사용하는 것을 방지하는데 사용되고, 모델이 더 새롭고 다양한 내용을 생성하게 할 수 있다.

- 모델이 이전에 사용한 단어나 구에 대해 페널티를 받게 되므로, LLM이 관련 없는 텍스트를 생성하는 것을 방지하는데 유용하다.


```
이러한 페널티가 토큰 생성에 영향을 미치게 된다. presence penalty는 특정 토큰의 사용을 억제하는 반면, frequency penalty는 토큰 사용을 장려한다. 
예를 들어, 언어 번역에서는 희귀한 단어가 더 자주 사용되도록 frequency penalty를 적용할 수 있게 된다.
```


### 6. max_tokens

- LLM이 생성하는 최대 토큰 갯수이다.

- 제한이 있어서 프롬프트 및 모델 완성에 사용할 수 있는 최대 토큰 수를 결정한다.

- 모델 LLM의 아키텍처에 따라 결정되며 한 번에 처리할 수 있는 최대 토큰을 나타낸다.

- 계산 비용과 메모리 요구 사항은 최대 토큰에 정비례 한다.


```
더 긴 최대 토큰을 설정하게 되면 더 큰 context와 일관성 있는 출력 텍스트를 얻을 수 있다.
최대 토큰을 짧게 설정하면 메모리를 덜 사용하고, 응답 속도는 빨라지지만 출력에 오류와 불일치가 발생하게 된다.
LLM을 훈련하고 파인튜닝하는 동안 최대 토큰이 설정되게 되는데, 출력 생성 중 토큰 길이를 파인튜닝 하는 것과는 달리,
출력의 일관성과 길이는 튜닝이 필요할 수 있는 다른 매개변수에 영향을 주지는 않고 특정 작업과 요구 사항을 기반으로 초기에 결정하는 것이 바람직하다.
```
 

 

### 7. stop

- 기본값은 null을 가지고, 문자열, 배열, null을 선택적으로 가진다.

- 모델이 지정된 단어나 구를 만날 경우 텍스트 생성을 중단하도록 하여, 특정 지점에서 출력을 종료할 수 있다.

- 단락이나 문장 측면에서 sequence의 끝을 의미하며, 최대 토큰과 유사하게 낮게 설정될 경우 추론 비용이 줄어든다.

- 생성 시 생성을 중지하는 문자열 목록이며, 반환된 출력에는 중지 문자열이 포함되지 않습니다.

 
```
Stop Sequence가 2로 설정되면 생성된 텍스트 or 출력이 단락으로 제한되고, 1로 설정되면 생성된 텍스트가 문장으로 제한된다.
```


### 8. best_of (Number of Candidiates 'n')

- 프롬프트에서 생성되는 출력 시퀀수 수이다. 'best_of' 시퀀스에서 상위 'n'개의 시퀀스를 반환한다.

- 'best_of'는 n 보다 크거나 같아야 한다.

- 'use_beam_serach'가 True일 때 빔 폭으로 처리되며, 기본적으로 'best_of'는 n으로 설정된다.

 
### 9. logprobs

- 기본적 형식으로 boolean 또는 null을 선택적으로 가진다.

- logprobs : True 이면 모델이 생성한 각 출력 토큰의 로그 확률을 반환하며, 모델의 확신도를 평가할 수 있다.

- 일반적으로 음수로 표현되며 0에 가까울수록 모델의 확신도가 높고, 큰 음수일수록 확신도가 낮다.


### 10. top_logprobs

- 기본적 형식으로 interger 또는 null을 선택적으로 가진다.

- 0~5 사이의 정수로, 각 토큰 위치에서 반환할 가능성이 가장 높은 토큰의 수를 지정한다.

- 각 토큰에는 연결된 로그 확률이 있다.


### 11. do_sample

- 다양성과 일관성을 결정할 수 있는 파라미터이다.


```
1) do_sample=True
모델이 다항 샘플링(모델이 각 단어에 대해 부여한 확률 분포를 기반으로 단어를 선택하는 방식)을 사용하여 다음 단어를 선택하게 된다.
모델이 더 창의적이고, 다양한 문장을 생성할 수 있다.

예측 가능성이 높은 단어뿐만 아니라, 덜 자주 등장하지만 문맥상 가능한 단어도 선택될 수 있다.
예측 불가능한 문장을 다양하게 생성할 수 있어, 창의적인 글쓰기, 시 등의 작업에 유리하다.

2) do_sample=False
모델이 일관성과 정확성이 높은 문장을 생성할 수 있도록 하지만, 다양성과 창의성은 감소될 수 있다.
기술적 내용, 정확한 정보 전달을 위한 작업에 유리하다.
```


### 12. messages

- 대화를 구성하는 메시지 list


### 13. logit_bias

- completion에 지정된 토큰이 나타날 확률을 수정한다.
- 선택적 기본값은 null이다.


### 14. n

- 각 input message에 대해 생성할 chat completion 선택 사항의 개
- cost를 최소화하려면 n=1을 유지한다.

### 15. response_format

- Json 모드의 구체적인 output 형식

```
Json 모드를 사용하도록 설정하려면

"type": "json_object"

이렇게 사용한다.
```

### 16. seed

- integer 또는 null을 선택적으로 가진다.

- 지정된 시드로 deterministic sampling을 보장하는 하이퍼파라미터.

- 지정한 경우, 시스템은 deterministic하게 샘플링하여 동일한 시드와 매개변수로 반복된 요청이 동일한 결과를 반환한다.

- Determinism은 보장되지 으며, 백엔드의 변경 사항을 모니터링 하기 위해서는 system_fingerprint 매개변수를 참조해야한다.


### 17. system_fingerprint

- string type을 가진다.

- fingerprint는 모델이 실행되는 백엔드 구성을 나타낸다.

-  seed 요청 매개변수와 함께 사용해서 Determinism에 영향을 줄 수 있는 백엔드 변화가 이루어진 시점을 파악할 수 있다.

```
{"system_fingerprint": "fp_44709d6fcb"}

이렇게 사용한다.
```


### 18. stream

- boolean 또는 null을 선택적으로 가지고, 기본값은 false이다.

- 옵션 설정 시 ChatGPT에서와 같이 부분적인 메세지 델타가 전송됨.

- 토큰이 사용 가능해지면 데이터 전용 서버 전송 이벤트로 전송되고, stream은 데이터로 종료된다.

- [DONE] 메시지로 stream이 종료된다.


### 19. tools

- 모델이 호출할 수 있는 함수 리스트(array 형태를 가짐)

- Json inputs를 생성할 수 있는 모델의 함수 리스트를 제공함.


### 20. tool_choice

- 모델에 의해서 어떤 함수가 호출될지를 컨트롤 하는 하이퍼파라미터.

- 기본적으로 string, object 형태를 선택적으로 가짐.

```
none : 모델이 함수를 호출하지 않고, 대신에 메시지를 생성할 것을 의미.
auto : 모델이 메시지를 생성할지 함수를 호출할지를 선택할 수 있다는 것을 의미.

none은 함수가 존재하지 않을 때
auto는 함수가 존재할 때
{"type": "function", "function": {"name": "my_function"}}

이렇게 사용
```


### 21. user

- 기본적으로 string 형태로 사용

- 최종 사용자를 나타내는 고유 식별자, openAI가 남용을 모니터링하고 감지하는데 도움을 준다.


[openAI API에서 사용될 수 있는 hyperparameter들의 code 활용법](https://cookbook.openai.com/examples/how_to_format_inputs_to_chatgpt_models)


# Hyperparameters fine-tuning

LLM에서 하이퍼파라미터 튜닝은 모델의 성능을 최적화하고, 특정 task나 목적에 맞게 모델의 output을 조절하는 과정이다.

주로 튜닝 시키는 하이퍼파라미터들의 종류들을 정리해보자.

### 1. temperature

- 가장 일반적으로 조정되는 하이퍼파라미터이다.
- Temperature를 높게 설정하면 창의적인 답변을, 낮게 설정하면 보수적인 답변을 내놓는 것을 위에서 정리했다. 이를 통해 모델이 생성하는 텍스트의 창의성과 무작위성을 조절할 수 있다.


```
일반적으로 0~1사이의 값을 가지지만, 가끔씩은 1을 초과하는 값도 사용된다.

낮은 temperature (0.0~0.3) : 예측이 가능하고 일관된 텍스트를 생성함.

중간 temperature (0.4~0.7) : 균형 잡힌 창의성과 예측 가능성을 제공함.

높은 temperature (0.7~1.0이상) : 매우 창의적이고 무작위적인 텍스트를 제공함. 1.0 이상의 값을 사용하게 되면, 예측 불가능한 정도의 창의적인 결과를 얻을 수 있지만, 때로는 덜 일관성 있거나 관련이 없는 내용이 포함될 수 있다.

일반적으로 높은 Temperature는 0.8 이상으로 간주된다.
```
 

### 2. max_tokens (Max Length)

- 생성될 텍스트의 길이를 결정한다.
- 짧은 응답이 필요한 경우에는 낮게, 더 긴 내용을 원할 경우에는 높게 설정한다.
- 계산 비용과 메모리 소모를 결정할 수 있는 핵심적인 하이퍼파라미터인 만큼 튜닝을 통해서 task의 성격에 적합하게 조절을 할 수 있다.


### 3. top-k, top-p Sampling

- 다양성과 예측 가능성을 조절할 수 있다.
- 샘플링 방법을 통해서 모델이 다음 토큰을 선택할 때의 후보군을 제한할 수 있다.
- Top-k보다 Top-p가 조금 더 자주 사용되는 하이퍼파라미터이고, Top-p를 높게 해서 텍스트의 다양성을 높일 수도 있고, 낮게 해서 정확하고 사실적인 답변을 생성하는 것을 기대할 수 있다.

```
Top-p의 값은 일반적으로 0~1 사이의 값을 가진다.

낮은 Top-p (0.0~0.5) : 일관되고 안정적인 텍스트를 생성함.

중간 Top-p (0.6~0.8) : 균형 잡힌 다양성과 일관성을 제공함.

높은 Top-p (0.9~1.0) : 매우 다양하고 예측 불가능한 텍스트를 생성함. 1.0에 가까울수록 모델은 가능한 모든 토큰을 고려하여 생성하게 됨.

일반적으로 높은 Top-p는 0.9 이상으로 간주된다.
```


### 4. stop

- 특정 단어나 구를 만나면 생성을 중단하도록 설정할 수 있다.

- 생성된 텍스트의 길이와 구조를 제어하는 데 도움을 주는 것으로, 특정 텍스트에서 생성을 중단하도록 할 수 있는 중요한 하이퍼파라미터이다.


### 5. prompt

- 모델에 제공되는 입력인 프롬프트의 구성 방식을 조정한다.

- 효과적인 프롬프트를 통해서 모델의 출력 품질을 크게 향상 시킬 수 있을 것이다.


### 6. model

- 사용할 모델의 버전과 종류를 선택한다.

- 다양한 모델이 서로 다른 성능과 특성을 가진 만큼 특정 task에 적합한 모델을 사용하는 것이 중요하다.


## hyperparameter 사용방식
```
import openai

# API 키 설정
openai.api_key = 'api-key'

# 텍스트 생성 요청
output = openai.Completion.create(
    model="davinci",                                           # 사용할 모델엔진 설정
    prompt="Translate the following English text to French:",  # 프롬프트 입력
    max_tokens=100,                                            # 생성할 최대 토큰 수
    temperature=0.7,                                           # 창의성과 예측 가능성 조절
    top_p=0.8,                                                 # 텍스트의 다양성 확보
    frequency_penalty=0.5,                                     # 반복되는 단어/구 사용 감소
    presence_penalty=0.5,                                      # 최근 사용된 단어/구 사용 감소
    stop=["\n\n"]                                              # 생성을 중단할 특정 시퀀스
)

# 생성된 텍스트 출력
print(output.choies[0].text)

사용모델, prompt를 입력하고, max_tokens으로 생성할 최대 토큰 수를 제시하고, temperature로 창의성 및 예측 가능성을 조절해주었다.
top-p를 0.8로 하여 80% 다양성을 나타내 주었고, frequency_penalty와 presence_penalty로 반복되는 단어/구, 최근 사용된 단어/구의 사용을 감소시켜주었다.
마지막으로 stop sequence를 사용하여 개행이 될 경우 생성을 중단하도록 만들어 주었다.
```
